@startuml CA4_C4_Container_View

title CA4 C4 Level 3 Container Diagram\nGPU → Local Processor → Kafka → Worker → DB (+ optional S3)

skinparam shadowing false
skinparam componentStyle rectangle
skinparam defaultFontName Arial
skinparam wrapWidth 220
skinparam maxMessageSize 140

' -------------------------------------------------------------------
' ACTORS
' -------------------------------------------------------------------
actor "Colab Notebook User" as COLAB_USER
actor "Developer / Operator" as DEV_USER

' -------------------------------------------------------------------
' SYSTEM: GOOGLE COLAB
' -------------------------------------------------------------------
package "Google Cloud (Colab)" as GCP {
  rectangle "Colab Notebook Producer\n(Python Notebook)" as COLAB_PRODUCER
}

' -------------------------------------------------------------------
' SYSTEM: DEV MACHINE
' -------------------------------------------------------------------
package "Developer Machine" as DEV_HOST {
  rectangle "Processor API (Local Dev)\nFastAPI / Python\nlocalhost:8000\n(behind ngrok in dev)" as PROC_LOCAL
}

' -------------------------------------------------------------------
' SYSTEM: AWS VPC
' -------------------------------------------------------------------
package "AWS VPC - CA4 Infrastructure" as AWS_VPC {

  rectangle "Bastion Host\nEC2 / SSH jumpbox\nOnly public entrypoint" as BASTION

  ' Edge K3s cluster
  package "Edge K3s Cluster" as EDGE_K3S {
    rectangle "Processor API (Edge – optional)\nFastAPI / Python\nK3s Deployment + Service\n(Not deployed in current CA4)" as API_EDGE <<future>>

    rectangle "Kafka Cluster\nStatefulSet / Kafka\nTopic: gpu-metadata" as KAFKA
    rectangle "Metadata Worker\nPython / K3s Deployment\nKafka consumer for gpu-metadata" as WORKER

    rectangle "Promtail\nK3s DaemonSet\nLog collector" as PROMTAIL
    rectangle "Prometheus\nK3s Deployment\nMetrics scraper" as PROM
    rectangle "Loki\nK3s Deployment\nLog store" as LOKI
    rectangle "Grafana\nK3s Deployment\nDashboards" as GRAFANA
  }

  ' Data services
  rectangle "Managed Mongo-Compatible DB\n(AWS DocumentDB or Mongo Atlas)\nDB = ca4, Collection = gpu_metadata" as DB
  rectangle "S3 Bucket (optional)\nRaw / enriched archives" as S3
}

' -------------------------------------------------------------------
' RELATIONSHIPS: DATA FLOW
' -------------------------------------------------------------------

' Colab user runs notebook
COLAB_USER --> COLAB_PRODUCER : Run GPU/TPU notebook\n+ extraction code

' Dev-mode primary path: Colab -> Local Processor API (via ngrok)
COLAB_PRODUCER --> PROC_LOCAL : POST /metadata\nJSON over HTTPS\n(dev: via ngrok URL)

' Optional future path: Colab -> Edge Processor API directly
COLAB_PRODUCER --> API_EDGE : (Future) POST /metadata\nHTTPS via Ingress / VPN

' Local Processor API publishes to Kafka via bastion + port-forward
PROC_LOCAL --> BASTION : SSH tunnel\nssh -L 9092:svc/kafka:9092 ...
BASTION --> KAFKA : Forwarded connection\ninside VPC

PROC_LOCAL --> KAFKA : Publish gpu-metadata\nbootstrap=localhost:9092

' Edge Processor API (future) publishes directly in-cluster
API_EDGE --> KAFKA : (Future) Publish gpu-metadata\nin-cluster

' Worker consumes from Kafka
KAFKA --> WORKER : Consume gpu-metadata\n(Kafka protocol, consumer group=worker)

' Worker writes to DB and S3
WORKER --> DB : Insert transformed metadata docs\nca4.gpu_metadata
WORKER --> S3 : Optional upload of\nraw/enriched artifacts

' Observability flows
WORKER --> PROM : /metrics (if enabled)
KAFKA --> PROM : Broker metrics\nvia exporter
DB --> PROM : DB metrics\nvia exporter (optional)

PROMTAIL --> LOKI : Ship pod logs
API_EDGE --> PROMTAIL : Pod logs (future)
WORKER --> PROMTAIL : Pod logs
KAFKA --> PROMTAIL : Pod logs

PROM --> GRAFANA : Metrics datasource
LOKI --> GRAFANA : Logs datasource

' Dev access
DEV_USER --> BASTION : SSH tunnel\n(bastion → K3s, Kafka, Grafana)
DEV_USER --> GRAFANA : View dashboards\nvia local tunnel\nhttp://localhost:3000

@enduml
